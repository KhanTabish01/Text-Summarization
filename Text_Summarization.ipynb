{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":479,"status":"ok","timestamp":1671066027737,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"kX2DUsXPPqHJ"},"outputs":[],"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","from nltk.corpus import stopwords\n","import time\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","from sklearn.model_selection import train_test_split\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671066028462,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"rjrFoLMyURWp"},"outputs":[],"source":["MAX_LENGTH = 1000\n","teacher_forcing_ratio = 0.5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     "]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671066028463,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"qXQPWhsoURWp"},"outputs":[],"source":["# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n","contractions = { \n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"i'd\": \"i would\",\n","\"i'll\": \"i will\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'll\": \"it will\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"needn't\": \"need not\",\n","\"oughtn't\": \"ought not\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"she'd\": \"she would\",\n","\"she'll\": \"she will\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"that'd\": \"that would\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there had\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'll\": \"they will\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'll\": \"we will\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"who'll\": \"who will\",\n","\"who's\": \"who is\",\n","\"won't\": \"will not\",\n","\"wouldn't\": \"would not\",\n","\"you'd\": \"you would\",\n","\"you'll\": \"you will\",\n","\"you're\": \"you are\"\n","}\n"]},{"cell_type":"markdown","source":["# Data processing "],"metadata":{"id":"8EmdXUaLCD4q"}},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671066028463,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"xD9v-2CPgFO9"},"outputs":[],"source":["import time\n","import math\n","\n","import matplotlib.pyplot as plt\n","plt.switch_backend('agg')\n","import matplotlib.ticker as ticker\n","import numpy as np\n","\n","\n","def showPlot(points):\n","    plt.figure()\n","    fig, ax = plt.subplots()\n","    # this locator puts ticks at regular intervals\n","    loc = ticker.MultipleLocator(base=0.2)\n","    ax.yaxis.set_major_locator(loc)\n","    plt.plot(points)\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n","\n","def clean_text(text, remove_stopwords = True):\n","#    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n","    \n","    # Convert words to lower case\n","    text = text.lower()\n","    \n","    # Replace contractions with their longer forms \n","    if True:\n","        text = text.split()\n","        new_text = []\n","        for word in text:\n","            if word in contractions:\n","                new_text.append(contractions[word])\n","            else:\n","                new_text.append(word)\n","        text = \" \".join(new_text)\n","    \n","    # Format words and remove unwanted characters\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n","    text = re.sub(r'\\<a href', ' ', text)\n","    text = re.sub(r'&amp;', '', text) \n","    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n","    text = re.sub(r'<br />', ' ', text)\n","    text = re.sub(r'\\'', ' ', text)\n","    \n","    # Optionally, remove stop words\n"," #   if remove_stopwords:\n"," #       text = text.split()\n"," #       stops = set(stopwords.words(\"english\"))\n"," #       text = [w for w in text if not w in stops]\n"," #       text = \" \".join(text)\n","\n","    return text\n","\n","def sort_by_len(reviews_df): #not using at the moment\n","    reviews_df['Summary_len']=reviews_df.apply(lambda row:len(row.Summary),axis=1)\n","    reviews_df['Text_len']=reviews_df.apply(lambda row:len(row.Text),axis=1)\n","    reviews_df = reviews_df.sort_values(by=['Text_len'])\n","    reviews_df.drop(['Text_len','Summary_len'],axis=1,inplace=True)\n","\n","def create_inp_op_pairs(texts,summaries,reverse=False):\n","    pairs =[]\n","    if len(summaries)!=len(texts):\n","        print('Length mismatch!')\n","    for i in range(len(texts)):\n","        txt_i = texts[i]\n","        if reverse:\n","            txt_i = ' '.join(reversed(txt_i.split(' ')))\n","        pairs.append([txt_i,summaries[i]])\n","    return pairs\n","\n","class Lang:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","            \n","# Turn a Unicode string to plain ASCII, thanks to\n","# https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","# Lowercase, trim, and remove non-letter characters\n","\n","\n","def normalizeString(s):\n","#     s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","\n","eng_prefixes = (\n","    \"i am \", \"i m \",\n","    \"he is\", \"he s \",\n","    \"she is\", \"she s \",\n","    \"you are\", \"you re \",\n","    \"we are\", \"we re \",\n","    \"they are\", \"they re \"\n",")\n","\n","\n","def filterPair(p):\n","    return len(p[0].split(' ')) < MAX_LENGTH and \\\n","        len(p[1].split(' ')) < MAX_LENGTH #and p[1].startswith(eng_prefixes)\n","\n","\n","def filterPairs(pairs):\n","    return [pair for pair in pairs if filterPair(pair)]\n","\n","def prepareData(X_train,y_train,reverse=False):\n","    pairs = create_inp_op_pairs(X_train,y_train,reverse=reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    lang = Lang('eng')\n","    for pair in pairs:\n","        lang.addSentence(pair[0])\n","        lang.addSentence(pair[1])\n","    print(\"Counted words:\",lang.n_words)\n","    return lang, pairs\n","\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(lang, pair[0])\n","    target_tensor = tensorFromSentence(lang, pair[1])\n","    return (input_tensor, target_tensor)"]},{"cell_type":"markdown","source":["# Endcoder Decoder Model "],"metadata":{"id":"kRUvknZ-Bw2M"}},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671066029015,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"LP5fL1CORm0x"},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","    \n","class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        output = self.embedding(input).view(1, 1, -1)\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"]},{"cell_type":"markdown","source":["# **Training **"],"metadata":{"id":"LnmtJEfWA6kK"}},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671066029016,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"3fJGgVb9R6Y_"},"outputs":[],"source":["def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device)\n","\n","    decoder_hidden = encoder_hidden\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    if use_teacher_forcing:\n","        # Teacher forcing: Feed the target as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            loss += criterion(decoder_output, target_tensor[di])\n","            decoder_input = target_tensor[di]  # Teacher forcing\n","\n","    else:\n","        # Without teacher forcing: use its own predictions as the next input\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","            loss += criterion(decoder_output, target_tensor[di])\n","            if decoder_input.item() == EOS_token:\n","                break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length\n","\n","def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n","    start = time.time()\n","    plot_losses = []\n","    print_loss_total = 0  # Reset every print_every\n","    plot_loss_total = 0  # Reset every plot_every\n","\n","    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n","    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n","    training_pairs = [tensorsFromPair(random.choice(pairs))\n","                      for i in range(n_iters)]\n","    criterion = nn.NLLLoss()\n","\n","    for iter in range(1, n_iters + 1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","\n","        loss = train(input_tensor, target_tensor, encoder,\n","                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        print_loss_total += loss\n","        plot_loss_total += loss\n","\n","        if iter % print_every == 0:\n","            print_loss_avg = print_loss_total / print_every\n","            print_loss_total = 0\n","            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n","                                         iter, iter / n_iters * 100, print_loss_avg))\n","\n","        if iter % plot_every == 0:\n","            plot_loss_avg = plot_loss_total / plot_every\n","            plot_losses.append(plot_loss_avg)\n","            plot_loss_total = 0\n","\n","    showPlot(plot_losses)\n","\n","\n","    \n","def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(lang, sentence)\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden()\n","\n","        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n","\n","        for ei in range(input_length):\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n","                                                     encoder_hidden)\n","            encoder_outputs[ei] += encoder_output[0, 0]\n","\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n","\n","        decoder_hidden = encoder_hidden\n","\n","        decoded_words = []\n","        decoder_attentions = torch.zeros(max_length, max_length)\n","\n","        for di in range(max_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs)\n","            decoder_attentions[di] = decoder_attention.data\n","            topv, topi = decoder_output.data.topk(1)\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(lang.index2word[topi.item()])\n","\n","            decoder_input = topi.squeeze().detach()\n","\n","        return decoded_words, decoder_attentions[:di + 1]\n","\n","def evaluateRandomly(encoder, decoder, n=10):\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words, attentions = evaluate(encoder, decoder, pair[0])\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')\n","\n","def showAttention(input_sentence, output_words, attentions):\n","    # Set up figure with colorbar\n","    fig = plt.figure()\n","    ax = fig.add_subplot(111)\n","    cax = ax.matshow(attentions.numpy(), cmap='bone')\n","    fig.colorbar(cax)\n","\n","    # Set up axes\n","    ax.set_xticklabels([''] + input_sentence.split(' ') +\n","                       ['<EOS>'], rotation=90)\n","    ax.set_yticklabels([''] + output_words)\n","\n","    # Show label at every tick\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","    plt.show()\n","\n","\n","def evaluateAndShowAttention(input_sentence):\n","    output_words, attentions = evaluate(\n","        encoder1, attn_decoder1, input_sentence)\n","    print('input =', input_sentence)\n","    print('output =', ' '.join(output_words))\n","    showAttention(input_sentence, output_words, attentions)"]},{"cell_type":"markdown","source":["# Importing dataset "],"metadata":{"id":"1uvTFkM3BILw"}},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13925,"status":"ok","timestamp":1671066042937,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"LZflP5moedAg","outputId":"54557e08-63fb-4b53-aa89-43d2119526b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":8351,"status":"ok","timestamp":1671066051286,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"7wLhDd_pURWu"},"outputs":[],"source":["reviews = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ANN Project/Text Summarizer/Reviews.csv\")"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1671066051288,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"BcCqeWeyURWu"},"outputs":[],"source":["keep_only_a_subset,subset_size = True, 10600\n","\n","if keep_only_a_subset:\n","    reviews = reviews[:subset_size]\n","    \n","SOS_token = 0\n","EOS_token = 1"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1671066051290,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"MmhQSLQ1URWu","outputId":"903fd431-de30-4b72-ec10-69426f23300f"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-26-f5a52172d03b>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n","  reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n"]},{"output_type":"execute_result","data":{"text/plain":["                 Summary                                               Text\n","0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n","1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n","2  \"Delight\" says it all  This is a confection that has been around a fe...\n","3         Cough Medicine  If you are looking for the secret ingredient i...\n","4            Great taffy  Great taffy at a great price.  There was a wid..."],"text/html":["\n","  <div id=\"df-ed2005a7-aca1-454d-a525-8bbaf46e7904\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Summary</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Good Quality Dog Food</td>\n","      <td>I have bought several of the Vitality canned d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Not as Advertised</td>\n","      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"Delight\" says it all</td>\n","      <td>This is a confection that has been around a fe...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Cough Medicine</td>\n","      <td>If you are looking for the secret ingredient i...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Great taffy</td>\n","      <td>Great taffy at a great price.  There was a wid...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed2005a7-aca1-454d-a525-8bbaf46e7904')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ed2005a7-aca1-454d-a525-8bbaf46e7904 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ed2005a7-aca1-454d-a525-8bbaf46e7904');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}],"source":["# Remove null values and unneeded features\n","reviews = reviews.dropna()\n","reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n","                        'Score','Time'], 1)\n","reviews = reviews.reset_index(drop=True)\n","reviews.head()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":686,"status":"ok","timestamp":1671066051968,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"RGpy4sw6URWu","outputId":"9ed4b9a3-a92b-4131-d978-e6b89afc7cce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaning data\n","Summaries are complete.\n","Texts are complete.\n","Took 0.5458869934082031 seconds 0.00909813642501831 minutes\n"]}],"source":["import multiprocessing\n","# Clean the summaries and texts\n","t1 = time.time()\n","pool = multiprocessing.Pool()\n","map_func = [pool.map,map][0]\n","print('Cleaning data')\n","clean_summaries = list(map_func(clean_text,reviews.Summary))\n","print(\"Summaries are complete.\")\n","\n","clean_texts = list(map_func(clean_text,reviews.Text))\n","pool.close()\n","print(\"Texts are complete.\")\n","print('Took',time.time()-t1,'seconds',(time.time()-t1)/60,'minutes')"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671066051969,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"-5Ooq8hFURWv","outputId":"a21bde82-8f43-4938-bf3c-f57bfe1e3348"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5962, 5962, 2650, 2650, 1988, 1988]"]},"metadata":{},"execution_count":28}],"source":["X_train,X_test,y_train,y_test = train_test_split(clean_texts,clean_summaries)\n","X_train,X_val,y_train,y_val = train_test_split(X_train,y_train)\n","\n","list(map(len,[X_train,y_train,X_test,y_test,X_val,y_val]))"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671066051969,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"dd_997idURWv","outputId":"b7be5cba-5637-4e5a-bf67-cf6a8f327707"},"outputs":[{"output_type":"stream","name":"stdout","text":["Read 5962 sentence pairs\n","Trimmed to 5957 sentence pairs\n","Counting words...\n","Counted words: 15607\n","['this coffee is good but  for me  it is nothing special  i will buy it again because it is a nice  mellow blend  and my favorite daily coffee  nabob  must now be shipped from canada  however  on my first order to the canadian company  it is taking a long time to be delivered  still have not gotten it  <br  ><br  >the reason for only three stars is not about the quality of the coffee but the fact that the description of the product does not mention the fact that this  kona coffee  is actually only 15  kona  if you look very carefully at the photo of the disc  you can see that information  and there is a legal disclaimer saying that the product may not match exactly with the description  so it is my fault for not reading more carefully  but i was happy to see what i thought was 100  kona and was not careful  i have bought the 15  kona before in a local store and did know that there was such a product in the tassimo discs  but i just read the product title and ordered <br  ><br  >as i said  the coffee tastes good  the amount of coffee is the usual amount  neither the small cups of some european coffees or the extra large size  and i would recommend the coffee  just be aware that this is blended coffee with only 15  kona  and do not expect it to come in a flash  as it did when amazon prime delivered it ', 'note  rating both coffee and seller']\n"]}],"source":["lang, pairs = prepareData(X_train,y_train,reverse=False)\n","print(random.choice(pairs))"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1671066052503,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"},"user_tz":-330},"id":"074rL8ulSi9M"},"outputs":[],"source":["hidden_size = 256\n","encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n","attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVuAS7pZTZtk","executionInfo":{"status":"ok","timestamp":1671071327548,"user_tz":-330,"elapsed":5275047,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"}},"outputId":"9ef2786a-c729-4560-804c-0cb990cf8b03"},"outputs":[{"output_type":"stream","name":"stdout","text":["1m 42s (- 83m 56s) (100 2%) 5.7486\n","3m 18s (- 79m 29s) (200 4%) 5.1913\n","5m 0s (- 78m 25s) (300 6%) 5.3316\n","6m 36s (- 75m 59s) (400 8%) 4.8987\n","8m 12s (- 73m 50s) (500 10%) 5.1013\n","9m 39s (- 70m 48s) (600 12%) 4.9225\n","11m 20s (- 69m 39s) (700 14%) 4.7221\n","13m 13s (- 69m 26s) (800 16%) 5.1239\n","14m 45s (- 67m 12s) (900 18%) 4.6019\n","16m 11s (- 64m 45s) (1000 20%) 4.3212\n","17m 53s (- 63m 25s) (1100 22%) 4.6293\n","19m 25s (- 61m 31s) (1200 24%) 4.9089\n","21m 14s (- 60m 27s) (1300 26%) 4.6989\n","22m 59s (- 59m 6s) (1400 28%) 4.3716\n","24m 45s (- 57m 46s) (1500 30%) 4.6394\n","26m 33s (- 56m 27s) (1600 32%) 4.5953\n","28m 14s (- 54m 49s) (1700 34%) 4.7397\n","29m 54s (- 53m 9s) (1800 36%) 4.3000\n","31m 19s (- 51m 6s) (1900 38%) 4.0977\n","33m 8s (- 49m 42s) (2000 40%) 4.4597\n","34m 45s (- 47m 59s) (2100 42%) 4.4621\n","36m 17s (- 46m 11s) (2200 44%) 4.4960\n","37m 49s (- 44m 23s) (2300 46%) 4.8759\n","39m 19s (- 42m 36s) (2400 48%) 4.5489\n","40m 57s (- 40m 57s) (2500 50%) 4.5030\n","42m 34s (- 39m 17s) (2600 52%) 4.1334\n","44m 2s (- 37m 31s) (2700 54%) 4.2228\n","45m 44s (- 35m 56s) (2800 56%) 4.2937\n","47m 41s (- 34m 32s) (2900 57%) 4.4367\n","49m 19s (- 32m 53s) (3000 60%) 4.3498\n","50m 58s (- 31m 14s) (3100 62%) 4.3456\n","52m 43s (- 29m 39s) (3200 64%) 4.4980\n","54m 27s (- 28m 3s) (3300 66%) 4.4687\n","55m 58s (- 26m 20s) (3400 68%) 4.4996\n","57m 46s (- 24m 45s) (3500 70%) 4.0922\n","59m 10s (- 23m 0s) (3600 72%) 4.3436\n","61m 0s (- 21m 26s) (3700 74%) 4.4566\n","62m 48s (- 19m 50s) (3800 76%) 4.4791\n","64m 23s (- 18m 9s) (3900 78%) 4.4932\n","66m 17s (- 16m 34s) (4000 80%) 4.2557\n","68m 16s (- 14m 59s) (4100 82%) 4.1530\n","70m 31s (- 13m 26s) (4200 84%) 4.3775\n","72m 37s (- 11m 49s) (4300 86%) 4.3948\n","75m 3s (- 10m 14s) (4400 88%) 4.6079\n","77m 8s (- 8m 34s) (4500 90%) 4.3028\n","79m 41s (- 6m 55s) (4600 92%) 4.1913\n","81m 52s (- 5m 13s) (4700 94%) 4.5074\n","84m 11s (- 3m 30s) (4800 96%) 4.3141\n","85m 59s (- 1m 45s) (4900 98%) 3.9082\n","87m 54s (- 0m 0s) (5000 100%) 4.3961\n"]}],"source":["trainIters(encoder1, attn_decoder1, 5000, print_every=100)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"TajAXRvrSyRw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671071328286,"user_tz":-330,"elapsed":753,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"}},"outputId":"01e49109-3c37-47cb-ee3b-ccc125b0b4ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["> i am amazed how many are being so easily fooled  what you are seeing is a generic ingredient list that covers multiple coffees  what it is saying  based on the photo i see above  is that if the coffee type you have is straight coffee  then all it contains is 100  arabica coffee    but if what you are drinking is a flavored coffee  then the ingredients in it are arabica coffee  and flavorings    but if what you are drinking is a decaf variety  then it contains arabica coffee that has been decaffeinated with ethyl acetate  as is standard <br  ><br  >so if you are not holding a decaf variety  then there is no decaffeinated beans  it is nothing more than generic catch all ingredients that apply to whichever variety you happen to be looking at <br  ><br  >sheesh  think people  lol\n","= it is not decaf people   \n","< great <EOS>\n","\n","> this product has a nice smooth taste with the additional tanginess of tangerine <br  >it also has a little bit of a sweetness to it but syrupy sweet <br  >not lingering or noticeable aftertaste \n","= a nice alternative to soda\n","< great the <EOS>\n","\n","> i did not enjoy this soup  i will try a different brand next time \n","= bad soup\n","< the best <EOS>\n","\n","> the individual pocky sticks are not separately packaged and melt and stick together while being shipped \n","= pocky sticks together\n","< great the <EOS>\n","\n","> we will see how these work out in our family over time  i am not sure that i am disciplined enough to make good use of them  although it might be a good technique for teaching my dog  they appear to be good for what they are  a very tiny  but healthy  reward  right now i can visualize them being used as a reward for following commands  a training tool  i was surprised at how tiny these treats were  they seem to taste good enough  but they are miniscule  they are just a fraction longer than a small paperclip and about half as wide  maybe 1 4 as thick <br  ><br  >my dog was a little annoyed  but took it when i gave it to him  that was yesterday  today he came in excited to get his  treat   looked at it as if to say   you ve got to be kidding   then he took it and ate it  a little later i offered him another one  he put it on the floor and looked hopefully at his stash of larger treats  but when he did not get one  he went back and took this one <br  ><br  >i think these might be good for rewards for obeying commands  treats  that is stretching it <br  ><br  >they may also be good for my son s dog who rarely gets treats because she is overweight and would eat the whole entire refrigerator full of food if allowed  at least she would get a treat  large treats take her 5 seconds  she will probably inhale these  one second max <br  ><br  >i like the fact that these are low fat  with no added salt  no refined sugar  no artificial flavors  colors or preservatives  it says   treats baked with natural peanut butter  \n","= teeny  tiny little treats  but good for what they are  a healthy reward for following commands\n","< the best <EOS>\n","\n","> a great coffee  that is medium roast  not bitter  yet it tastes as good as it smells  it is also well bagged to maintain optimum freshness mmmmmmm\n","= jeremiahs organic breakfast blend \n","< great the <EOS>\n","\n","> i thought this coffee was really horrible  had one cup and will throw the rest away because it cannot be returned \n","= taste like old socks\n","< the best <EOS>\n","\n","> love this tea   especially love triangle bags more flavor   the tea is what i remember it to be   happy<br  >came right on time in great shape   happily enjoying my pg tips    \n","= who does not love pg tips\n","< great the <EOS>\n","\n","> my husband and both love almonds  i have even shared this snack with co workers and everyone is raving about the good taste of this snack  i am taking a few bags on our next backpacking trip \n","= great almond crunch  \n","< great the <EOS>\n","\n","> yay an all natural kids gum   no bht preservative   no dangerous artificial sweetners   tastes fine and not bad for you  \n","= all natural ~        \n","< great the <EOS>\n","\n"]}],"source":["evaluateRandomly(encoder1, attn_decoder1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W86DDi-cURWx","executionInfo":{"status":"aborted","timestamp":1671071328291,"user_tz":-330,"elapsed":10,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfvyltngURWx","executionInfo":{"status":"aborted","timestamp":1671071328291,"user_tz":-330,"elapsed":10,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_f8857YoURWx","executionInfo":{"status":"aborted","timestamp":1671071328291,"user_tz":-330,"elapsed":10,"user":{"displayName":"TABISH KHAN","userId":"06715832557578727146"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}